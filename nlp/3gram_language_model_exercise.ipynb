{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3gram-language-model-exercise.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPWh8LxHS6hsNbIVYbGZg5o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cbadenes/notebooks/blob/main/nlp/3gram_language_model_exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build a Tri-gram model from the Reuters corpus.\n",
        "The [Reuters Corpus](https://www.nltk.org/book/ch02.html) contains 10,788 news documents totaling 1.7 million words. The documents have been classified into 90 topics, and grouped into two sets, called \"training\" and \"test\"; thus, the text with fileid 'test/14826' is a document drawn from the test set. This split is for training and testing algorithms that automatically detect the topic of a document, as we will see in chap-data-intensive.\n",
        "\n",
        "This notebook is based on the exercise proposed [here](https://nlpforhackers.io/language-models/).\n",
        "\n",
        "The notebook with the solution is available [here](https://github.com/cbadenes/notebooks/blob/main/nlp/3gram_language_model.ipynb)"
      ],
      "metadata": {
        "id": "NvB80xdxowtN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1) Load Data"
      ],
      "metadata": {
        "id": "j66qCeqvthlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "from nltk import bigrams, trigrams\n",
        "nltk.download('reuters')\n",
        "nltk.download('punkt')\n",
        "!unzip -o -q /root/nltk_data/corpora/reuters.zip -d /root/nltk_data/corpora\n",
        "from nltk.corpus import reuters\n",
        "\n",
        "\n",
        "def get_bigrams(sentence,pad=False):\n",
        " return list(bigrams(sentence,pad_left=pad, pad_right=pad))\n",
        "\n",
        "def get_trigrams(sentence,pad=False):\n",
        " return list(trigrams(sentence,pad_left=pad, pad_right=pad))\n",
        "\n",
        "\n",
        "print(\"counting words..\")\n",
        "total_count = len(reuters.words())\n",
        "print(\"Total Words:\", total_count)\n",
        "counts = Counter(reuters.words())\n",
        "print(\"Top5 most common words:\", counts.most_common(n=5))\n",
        "\n"
      ],
      "metadata": {
        "id": "w_8WDtnXpFgv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}