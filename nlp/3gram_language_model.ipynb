{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3gram-language-model.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "j66qCeqvthlt",
        "qgZDEhvitySP",
        "00msNtL5xYES",
        "cu-7Tdvf6-Z8"
      ],
      "authorship_tag": "ABX9TyP2/sIfCP/bzqpS4+ii3FxB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cbadenes/notebooks/blob/main/nlp/3gram_language_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tri-gram model from the Reuters corpus.\n",
        "The [Reuters Corpus](https://www.nltk.org/book/ch02.html) contains 10,788 news documents totaling 1.7 million words. The documents have been classified into 90 topics, and grouped into two sets, called \"training\" and \"test\"; thus, the text with fileid 'test/14826' is a document drawn from the test set. This split is for training and testing algorithms that automatically detect the topic of a document, as we will see in chap-data-intensive.\n",
        "\n",
        "This notebook is based on the exercise proposed [here](https://nlpforhackers.io/language-models/)."
      ],
      "metadata": {
        "id": "NvB80xdxowtN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1) Load Data"
      ],
      "metadata": {
        "id": "j66qCeqvthlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk import bigrams, trigrams\n",
        "nltk.download('reuters')\n",
        "nltk.download('punkt')\n",
        "!unzip -o -q /root/nltk_data/corpora/reuters.zip -d /root/nltk_data/corpora\n",
        "from nltk.corpus import reuters\n",
        "\n",
        "\n",
        "def get_bigrams(sentence,pad=False):\n",
        " return list(bigrams(sentence,pad_left=pad, pad_right=pad))\n",
        "\n",
        "def get_trigrams(sentence,pad=False):\n",
        " return list(trigrams(sentence,pad_left=pad, pad_right=pad))\n",
        "\n",
        "\n",
        "print(\"counting words..\")\n",
        "total_count = len(reuters.words())\n",
        "print(\"Total Words:\", total_count)\n",
        "counts = Counter(reuters.words())\n",
        "print(\"Top5 most common words:\", counts.most_common(n=5))\n",
        "\n"
      ],
      "metadata": {
        "id": "w_8WDtnXpFgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2) Get n-grams"
      ],
      "metadata": {
        "id": "qgZDEhvitySP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the bigrams"
      ],
      "metadata": {
        "id": "leRJpV1iwj7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence=\"Natural language processing is a subfield of linguistics, computer science, and artificial intelligence\"\n",
        "\n",
        "get_bigrams(sentence.split(\" \"))"
      ],
      "metadata": {
        "id": "3KyEW6kGu9s_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the padded bigrams"
      ],
      "metadata": {
        "id": "Dh0wLY1-wwEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_bigrams(sentence.split(\" \"),pad=True)"
      ],
      "metadata": {
        "id": "NslRFFhIwyHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the trigrams"
      ],
      "metadata": {
        "id": "Y7jFVaDFxC5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_trigrams(sentence.split(\" \"))"
      ],
      "metadata": {
        "id": "Y2rzScggxEvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the padded trigrams"
      ],
      "metadata": {
        "id": "SlIDT6WjxJqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_trigrams(sentence.split(\" \"),pad=True)"
      ],
      "metadata": {
        "id": "vH2WlFXbxLf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3) Count occurrences\n"
      ],
      "metadata": {
        "id": "00msNtL5xYES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "for sentence in reuters.sents():\n",
        "    for w1, w2, w3 in get_trigrams(sentence, pad=True):\n",
        "        model[(w1, w2)][w3] += 1\n",
        "print(\"Total bi-grams:\",len(model))"
      ],
      "metadata": {
        "id": "cx2DY96Yxasq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " how many times \"economists\" follows \"what the\"?"
      ],
      "metadata": {
        "id": "UuNO4HBR57xY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model[\"what\", \"the\"][\"economists\"]"
      ],
      "metadata": {
        "id": "JpsjzLXW58aN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "and a \"nonexistingword\"?"
      ],
      "metadata": {
        "id": "xX_GcjsR6Bf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model[\"what\", \"the\"][\"nonexistingword\"])"
      ],
      "metadata": {
        "id": "lAbmII8J6GE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "how many sentences start with \"The\"?"
      ],
      "metadata": {
        "id": "DJSzNtQ66PiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model[None, None][\"The\"]"
      ],
      "metadata": {
        "id": "zDCG7k7t6Sja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's transform the counts to probabilities"
      ],
      "metadata": {
        "id": "9MGGPbpm6iOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for w1_w2 in model:\n",
        "    total_count = float(sum(model[w1_w2].values()))\n",
        "    for w3 in model[w1_w2]:\n",
        "        model[w1_w2][w3] /= total_count\n",
        "print(\"done!\")"
      ],
      "metadata": {
        "id": "KlcFg3f-6i27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the probability that \"economists\" follows \"what the\"?"
      ],
      "metadata": {
        "id": "vn3efiZs6tUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model[\"what\", \"the\"][\"economists\"]"
      ],
      "metadata": {
        "id": "cYhYrXca6wkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "and a sentence starts with \"The\"?"
      ],
      "metadata": {
        "id": "LoHkB0OE62JW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model[None, None][\"The\"]"
      ],
      "metadata": {
        "id": "IGj7ACmZ65Kw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4) Generate text"
      ],
      "metadata": {
        "id": "cu-7Tdvf6-Z8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are the most probable words to follow \"a company\"?"
      ],
      "metadata": {
        "id": "tibBqJCp-fqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = model[\"The\",\"market\"]\n",
        "for word in sorted(words, key=words.get, reverse=True)[:5]:\n",
        "    print(word, words[word])"
      ],
      "metadata": {
        "id": "ZEZ2ESdt9JcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a random sentence"
      ],
      "metadata": {
        "id": "cT05ANSeC3Re"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "text = [\"The\", \"market\"] \n",
        "#text = [None, None] \n",
        " \n",
        "sentence_finished = False\n",
        " \n",
        "while not sentence_finished:\n",
        "    r = random.random()\n",
        "    accumulator = .0\n",
        " \n",
        "    for word in model[tuple(text[-2:])].keys():\n",
        "        accumulator += model[tuple(text[-2:])][word]\n",
        " \n",
        "        if accumulator >= r:\n",
        "            text.append(word)\n",
        "            break\n",
        " \n",
        "    if text[-2:] == [None, None]:\n",
        "        sentence_finished = True\n",
        " \n",
        "print(' '.join([t for t in text if t]))"
      ],
      "metadata": {
        "id": "_EhvSrYC7BuR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}