{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3gram-language-model.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "j66qCeqvthlt",
        "qgZDEhvitySP",
        "00msNtL5xYES"
      ],
      "authorship_tag": "ABX9TyNV8Va39AFC/ALIpBKnFLyM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cbadenes/notebooks/blob/main/nlp/3gram_language_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tri-gram model from the Reuters corpus.\n",
        "The [Reuters Corpus](https://www.nltk.org/book/ch02.html) contains 10,788 news documents totaling 1.7 million words. The documents have been classified into 90 topics, and grouped into two sets, called \"training\" and \"test\"; thus, the text with fileid 'test/14826' is a document drawn from the test set. This split is for training and testing algorithms that automatically detect the topic of a document, as we will see in chap-data-intensive.\n",
        "\n",
        "This notebook is based on the exercise proposed [here](https://nlpforhackers.io/language-models/)."
      ],
      "metadata": {
        "id": "NvB80xdxowtN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1) Load Data"
      ],
      "metadata": {
        "id": "j66qCeqvthlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import pprint\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "nltk.download('reuters')\n",
        "nltk.download('punkt')\n",
        "!unzip -o -q /root/nltk_data/corpora/reuters.zip -d /root/nltk_data/corpora\n",
        "from nltk.corpus import reuters\n",
        "print(\"counting words..\")\n",
        "total_count = len(reuters.words())\n",
        "print(\"Number of words in corpus:\",total_count)"
      ],
      "metadata": {
        "id": "w_8WDtnXpFgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most common 5 words are ..."
      ],
      "metadata": {
        "id": "WFhQkC32sNef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "counts = Counter(reuters.words())\n",
        "print(counts.most_common(n=5))"
      ],
      "metadata": {
        "id": "_gpc_7qZsBDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2) Get n-grams"
      ],
      "metadata": {
        "id": "qgZDEhvitySP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import bigrams, trigrams\n",
        " \n",
        "sentence = reuters.sents()[50]\n",
        "print(sentence)"
      ],
      "metadata": {
        "id": "nPzKVYcrt0Qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the bigrams"
      ],
      "metadata": {
        "id": "leRJpV1iwj7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(bigrams(sentence)))"
      ],
      "metadata": {
        "id": "3KyEW6kGu9s_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the padded bigrams"
      ],
      "metadata": {
        "id": "Dh0wLY1-wwEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(bigrams(sentence, pad_left=True, pad_right=True))) "
      ],
      "metadata": {
        "id": "NslRFFhIwyHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the trigrams"
      ],
      "metadata": {
        "id": "Y7jFVaDFxC5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(trigrams(sentence)))"
      ],
      "metadata": {
        "id": "Y2rzScggxEvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the padded trigrams"
      ],
      "metadata": {
        "id": "SlIDT6WjxJqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(trigrams(sentence, pad_left=True, pad_right=True)))"
      ],
      "metadata": {
        "id": "vH2WlFXbxLf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3) Count the 3-grams"
      ],
      "metadata": {
        "id": "00msNtL5xYES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "for sentence in reuters.sents():\n",
        "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
        "        model[(w1, w2)][w3] += 1"
      ],
      "metadata": {
        "id": "cx2DY96Yxasq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " \"economists\" follows \"what the\" 2 times"
      ],
      "metadata": {
        "id": "UuNO4HBR57xY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model[\"what\", \"the\"][\"economists\"])"
      ],
      "metadata": {
        "id": "JpsjzLXW58aN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"nonexistingword\" 0 times"
      ],
      "metadata": {
        "id": "xX_GcjsR6Bf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model[\"what\", \"the\"][\"nonexistingword\"])"
      ],
      "metadata": {
        "id": "lAbmII8J6GE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "sentences start with \"The\""
      ],
      "metadata": {
        "id": "DJSzNtQ66PiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model[None, None][\"The\"])"
      ],
      "metadata": {
        "id": "zDCG7k7t6Sja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's transform the counts to probabilities"
      ],
      "metadata": {
        "id": "9MGGPbpm6iOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for w1_w2 in model:\n",
        "    total_count = float(sum(model[w1_w2].values()))\n",
        "    for w3 in model[w1_w2]:\n",
        "        model[w1_w2][w3] /= total_count"
      ],
      "metadata": {
        "id": "KlcFg3f-6i27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"economists\" follows \"what the\""
      ],
      "metadata": {
        "id": "vn3efiZs6tUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model[\"what\", \"the\"][\"economists\"])"
      ],
      "metadata": {
        "id": "cYhYrXca6wkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "sentences start with \"The\""
      ],
      "metadata": {
        "id": "LoHkB0OE62JW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model[None, None][\"The\"])"
      ],
      "metadata": {
        "id": "IGj7ACmZ65Kw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4) Generate some text"
      ],
      "metadata": {
        "id": "cu-7Tdvf6-Z8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "most probable word to \"today the\".."
      ],
      "metadata": {
        "id": "tibBqJCp-fqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = model[\"company\",\"would\"]\n",
        "for w in sorted(x, key=x.get, reverse=True):\n",
        "    print(w, x[w])"
      ],
      "metadata": {
        "id": "ZEZ2ESdt9JcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "#text = [\"Today\", \"the\"] \n",
        "text = [None, None] \n",
        " \n",
        "sentence_finished = False\n",
        " \n",
        "while not sentence_finished:\n",
        "    r = random.random()\n",
        "    accumulator = .0\n",
        " \n",
        "    for word in model[tuple(text[-2:])].keys():\n",
        "        accumulator += model[tuple(text[-2:])][word]\n",
        " \n",
        "        if accumulator >= r:\n",
        "            text.append(word)\n",
        "            break\n",
        " \n",
        "    if text[-2:] == [None, None]:\n",
        "        sentence_finished = True\n",
        " \n",
        "print(' '.join([t for t in text if t]))"
      ],
      "metadata": {
        "id": "_EhvSrYC7BuR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}