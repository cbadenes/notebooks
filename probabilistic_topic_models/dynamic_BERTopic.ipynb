{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dynamic-BERTopic.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNlHwsKQAxXjiwIR6R+epC4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cbadenes/notebooks/blob/main/probabilistic_topic_models/dynamic_BERTopic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notebook based on code retrieved from [MaartenGr/BERTopic](https://github.com/MaartenGr/BERTopic)\n",
        "\n",
        "Remember to enable GPU by `Runtime>Change runtime type>Hardware accelerator (GPU)`"
      ],
      "metadata": {
        "id": "B2BuZnc2eQtu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1PUyWkfeKbx"
      },
      "outputs": [],
      "source": [
        "!pip install bertopic"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we need to load in the data and do some very basic cleaning. For example, I am not interested in his re-tweets for this use-case:"
      ],
      "metadata": {
        "id": "h5g0xSxVekM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Prepare data\n",
        "trump = pd.read_csv('https://drive.google.com/uc?export=download&id=1xRKHaP-QwACMydlDnyFPEaFdtskJuBa6')\n",
        "trump.text = trump.apply(lambda row: re.sub(r\"http\\S+\", \"\", row.text).lower(), 1)\n",
        "trump.text = trump.apply(lambda row: \" \".join(filter(lambda x:x[0]!=\"@\", row.text.split())), 1)\n",
        "trump.text = trump.apply(lambda row: \" \".join(re.sub(\"[^a-zA-Z]+\", \" \", row.text).split()), 1)\n",
        "trump = trump.loc[(trump.isRetweet == \"f\") & (trump.text != \"\"), :]\n",
        "timestamps = trump.date.to_list()\n",
        "tweets = trump.text.to_list()\n",
        "print(len(tweets),\"tweets are ready!\")"
      ],
      "metadata": {
        "id": "CVRECVb9emiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some tweets..."
      ],
      "metadata": {
        "id": "21SCJB9KfCB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for tweet in tweets[:10]:\n",
        "  print(\"> Tweet:\",tweet)"
      ],
      "metadata": {
        "id": "FpCAh-anfGPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we need to extract the global topic representations by simply creating and training a BERTopic model:"
      ],
      "metadata": {
        "id": "-QNHsdrreqHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic import BERTopic\n",
        "\n",
        "topic_model = BERTopic(min_topic_size=35, verbose=True)\n",
        "topics, _ = topic_model.fit_transform(tweets)"
      ],
      "metadata": {
        "id": "vVccHeR9esHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then extract most frequent topics:"
      ],
      "metadata": {
        "id": "B_omn72ggEiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.get_topic_info().head(10)"
      ],
      "metadata": {
        "id": "MsCZYptegGcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topics over Time"
      ],
      "metadata": {
        "id": "sbykKeGYgVjM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From these topics, we are going to generate the topic representations at each timestamp for each topic. \n",
        "\n",
        "We do this by simply calling topics_over_time and pass in his tweets, the corresponding timestamps, and the related topics.\n",
        "\n",
        "There are a few important parameters that you should take note of, namely:\n",
        "\n",
        "* `docs`\n",
        "  * These are the tweets that we are using\n",
        "* `topics`\n",
        "  * The topics that we have created before\n",
        "* `timestamps`\n",
        "  * The timestamp of each tweet/document\n",
        "* `global_tuning`\n",
        "  * Whether to average the topic representation of a topic at time *t* with its global topic representation\n",
        "* `evolution_tuning`\n",
        "  * Whether to average the topic representation of a topic at time *t* with the topic representation of that topic at time *t-1*\n",
        "* `nr_bins`\n",
        "  * The number of bins to put our timestamps into. It is computationally inefficient to extract the topics at thousands of different timestamps. Therefore, it is advised to keep this value below 20. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PxXJnA27eumr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topics_over_time = topic_model.topics_over_time(docs=tweets, \n",
        "                                                topics=topics, \n",
        "                                                timestamps=timestamps, \n",
        "                                                global_tuning=True, \n",
        "                                                evolution_tuning=True, \n",
        "                                                nr_bins=20)"
      ],
      "metadata": {
        "id": "xiPZZ8lsewcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize Topics over Time\n",
        "\n",
        "After having created our topics_over_time, we will have to visualize those topics as accessing them becomes a bit more difficult with the added temporal dimension.\n",
        "\n",
        "\n",
        "To do so, we are going to visualize the distribution of topics over time based on their frequency. Doing so allows us to see how the topics have evolved over time. Make sure to hover over any point to see how the topic representation at time t differs from the global topic representation."
      ],
      "metadata": {
        "id": "_8pFvHNvezUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.visualize_topics_over_time(topics_over_time, top_n_topics=20)"
      ],
      "metadata": {
        "id": "VFs9CpPtg03o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}