{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LabeledLDA-Topics.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNvOk5qME2YHnWZkVilTjv0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cbadenes/notebooks/blob/main/probabilistic_topic_models/LabeledLDA_Topics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Use a Python implementation of Labeled-LDA based on Gibbs sampling\n",
        " from [JoeZJH](https://github.com/JoeZJH/Labeled-LDA-Python)"
      ],
      "metadata": {
        "id": "M_mMJJTO7e0z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtF6fdDX7Y7o"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "!git clone https://github.com/JoeZJH/Labeled-LDA-Python\n",
        "os.chdir('Labeled-LDA-Python/')\n",
        "!ls -la"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize a Labeled Topic Model\n",
        "* `alpha`: doc-topics ratio\n",
        "* `eta`: topic-terms ratio"
      ],
      "metadata": {
        "id": "8SrLo7Lz9los"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import model.labeled_lda as llda\n",
        "\n",
        "# initialize data\n",
        "labeled_documents = [(\"example example example example example\"*10, [\"example\"]),\n",
        "                     (\"test llda model test llda model test llda model\"*10, [\"test\", \"llda_model\"]),\n",
        "                     (\"example test example test example test example test\"*10, [\"example\", \"test\"]),\n",
        "                     (\"good perfect good good perfect good good perfect good \"*10, [\"positive\"]),\n",
        "                     (\"bad bad down down bad bad down\"*10, [\"negative\"])]\n",
        "\n",
        "# new a Labeled LDA model\n",
        "# llda_model = llda.LldaModel(labeled_documents=labeled_documents, alpha_vector=\"50_div_K\", eta_vector=0.001)\n",
        "# llda_model = llda.LldaModel(labeled_documents=labeled_documents, alpha_vector=0.02, eta_vector=0.002)\n",
        "llda_model = llda.LldaModel(labeled_documents=labeled_documents, alpha_vector=0.01, eta_vector=0.001)\n",
        "print(llda_model)"
      ],
      "metadata": {
        "id": "H410Bl-I8DaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train "
      ],
      "metadata": {
        "id": "i79RC2SZ9pt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training\n",
        "# llda_model.training(iteration=10, log=True)\n",
        "while True:\n",
        "    print(\"iteration %s sampling...\" % (llda_model.iteration + 1))\n",
        "    llda_model.training(1)\n",
        "    print(\"after iteration: %s, perplexity: %s\" % (llda_model.iteration, llda_model.perplexity()))\n",
        "    print(\"delta beta: %s\" % llda_model.delta_beta)\n",
        "    if llda_model.is_convergent(method=\"beta\", delta=0.01):\n",
        "        break"
      ],
      "metadata": {
        "id": "D6qPPIMQ8Jv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Update an existing model with new documents"
      ],
      "metadata": {
        "id": "qveDYzYd9zU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# update\n",
        "print(\"before updating: \", llda_model)\n",
        "update_labeled_documents = [(\"new example test example test example test example test\", [\"example\", \"test\"])]\n",
        "llda_model.update(labeled_documents=update_labeled_documents)\n",
        "print(\"after updating: \", llda_model)"
      ],
      "metadata": {
        "id": "qYaIEeEU8OQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train again\n",
        "# llda_model.training(iteration=10, log=True)\n",
        "while True:\n",
        "    print(\"iteration %s sampling...\" % (llda_model.iteration + 1))\n",
        "    llda_model.training(1)\n",
        "    print(\"after iteration: %s, perplexity: %s\" % (llda_model.iteration, llda_model.perplexity()))\n",
        "    print(\"delta beta: %s\" % llda_model.delta_beta)\n",
        "    if llda_model.is_convergent(method=\"beta\", delta=0.01):\n",
        "        break"
      ],
      "metadata": {
        "id": "URo7ddNS8S-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explore Topics"
      ],
      "metadata": {
        "id": "YSfhyVy5_G-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for topic in llda_model.topic_vocabulary.keys():\n",
        "  print(\"Topic '\",topic,\"' :\")\n",
        "  for term in llda_model.top_terms_of_topic(topic,5):\n",
        "    print(\"\\t-\",term)"
      ],
      "metadata": {
        "id": "m5I061wC_IRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference\n",
        "\n",
        "* `document`: some sentence like \"this is a method for inference\"\n",
        "* `times`: the number of samples of the target distribution (one whole iteration(sample for all words) generates a sample)\n",
        "* `iteration`: the times of iteration until Markov chain converges"
      ],
      "metadata": {
        "id": "HNYF5hh6-AaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# inference\n",
        "# note: the result topics may be different for difference training, because gibbs sampling is a random algorithm\n",
        "document = \"example llda model example example good perfect good perfect good perfect\" * 100\n",
        "\n",
        "topics = llda_model.inference(document=document, iteration=100, times=10)\n",
        "print(topics)"
      ],
      "metadata": {
        "id": "UgN6hkkQ8V2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "d1mzPnwn-DxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# perplexity\n",
        "# calculate perplexity on test data\n",
        "perplexity = llda_model.perplexity(documents=[\"example example example example example\",\n",
        "                                              \"test llda model test llda model test llda model\",\n",
        "                                              \"example test example test example test example test\",\n",
        "                                              \"good perfect good good perfect good good perfect good\",\n",
        "                                              \"bad bad down down bad bad down\"],\n",
        "                                   iteration=30,\n",
        "                                   times=10)\n",
        "print(\"perplexity on test data: %s\" % perplexity)\n",
        "# calculate perplexity on training data\n",
        "print(\"perplexity on training data: %s\" % llda_model.perplexity())"
      ],
      "metadata": {
        "id": "QLG4UbhH8c3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Model"
      ],
      "metadata": {
        "id": "jIDHBja1-GTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save to disk\n",
        "save_model_dir = \"data/model\"\n",
        "# llda_model.save_model_to_dir(save_model_dir, save_derivative_properties=True)\n",
        "llda_model.save_model_to_dir(save_model_dir)"
      ],
      "metadata": {
        "id": "IMvQz-KK8gCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model"
      ],
      "metadata": {
        "id": "t6xZdbRA-H0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load from disk\n",
        "llda_model_new = llda.LldaModel()\n",
        "llda_model_new.load_model_from_dir(save_model_dir, load_derivative_properties=False)\n",
        "print(\"llda_model_new\", llda_model_new)\n",
        "print(\"llda_model\", llda_model)\n",
        "print(\"Top-5 terms of topic 'negative': \", llda_model.top_terms_of_topic(\"negative\", 5, False))\n",
        "print(\"Doc-Topic Matrix: \\n\", llda_model.theta)\n",
        "print(\"Topic-Term Matrix: \\n\", llda_model.beta)"
      ],
      "metadata": {
        "id": "qDQAlPOu8jfE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}