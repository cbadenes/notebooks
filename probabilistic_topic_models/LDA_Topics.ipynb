{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LDA-Topics.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cbadenes/notebooks/blob/main/probabilistic_topic_models/LDA_Topics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVx3oh5JCi_F"
      },
      "source": [
        "This Notebook serves as an introduction to **Probabilistic Topic Models**. \n",
        "\n",
        "Textual data is loaded from a Google Sheet and topics derived from LDA will be generated. \n",
        "\n",
        "First we need to obtain credentials from our Google Account to access the corpus hosted on Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcubynMaDbzt"
      },
      "source": [
        "!pip install --upgrade -q gspread\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBOg8tF0dBC1"
      },
      "source": [
        "### Loading Data\n",
        "\n",
        "[CORDIS](https://cordis.europa.eu/projects/en) is the primary source of results from EU-funded projects since 1990. \n",
        "\n",
        "We used a [sample stored in a spreadsheet](https://goo.gl/dG8eVF) with 100 such projects. Save a copy in your `Google Colab/` folder at Google Drive.\n",
        "\n",
        "A dataframe will be created from the training google sheet:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXu9rcR2Dmz7"
      },
      "source": [
        "from google.colab import data_table\n",
        "data_table.enable_dataframe_formatter()\n",
        "\n",
        "# name of the spreadsheet with texts (e.g.'texts_nlp')\n",
        "corpus = 'texts_nlp'\n",
        "\n",
        "gc = gspread.authorize(GoogleCredentials.get_application_default())\n",
        "\n",
        "worksheet = gc.open(corpus).sheet1\n",
        "\n",
        "# get_all_values gives a list of rows.\n",
        "rows = worksheet.get_all_values()\n",
        "            \n",
        "# Convert to a DataFrame and render.\n",
        "import pandas as pd\n",
        "dataset_df = pd.DataFrame.from_records(rows[1:], columns=[\"ID\",\"TITLE\",\"DESCRIPTION\"])\n",
        "data_table.DataTable(dataset_df, include_index=False, num_rows_per_page=5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChKtbgQldfTf"
      },
      "source": [
        "### Data Cleaning\n",
        "\n",
        "Now, let`s create the BoWs by using the [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) facility. \n",
        "\n",
        "It is a [ScikitLearn](https://scikit-learn.org/stable/index.html) module focused to create bag-of-words from strings. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwiEHyUEdhOl"
      },
      "source": [
        "# from sklearn.feature_extraction import text \n",
        "# my_additional_stop_words = ['xxx','yyy']\n",
        "# my_stop_words = text.ENGLISH_STOP_WORDS.union(my_additional_stop_words)\n",
        "\n",
        "# list of texts\n",
        "documents = dataset_df['DESCRIPTION'].tolist()\n",
        "\n",
        "# bag-of-words\n",
        "tf_vectorizer = CountVectorizer(\n",
        "    stop_words=[],\n",
        "    min_df=1,\n",
        "    max_df=1.0,\n",
        "    lowercase=True,\n",
        "    max_features=50000,\n",
        "    token_pattern='[a-zA-Z0-9]{3,}',  \n",
        "    analyzer = 'word'\n",
        ")\n",
        "bag_of_words = tf_vectorizer.fit_transform(documents)\n",
        "dictionary = tf_vectorizer.get_feature_names()\n",
        "vocabulary = tf_vectorizer.vocabulary_\n",
        "\n",
        "print(\"Vocabulary Size: \", len(dictionary))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUcIcUukgaxV"
      },
      "source": [
        "Sorted list of terms by frequency:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIag37aMgeuz"
      },
      "source": [
        "s = bag_of_words.toarray().sum(axis=0)\n",
        "st = sorted(range(len(s)), key=lambda k: s[k], reverse=True)\n",
        "for i,x in enumerate(st[:20]):\n",
        "  print(dictionary[x],s[x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmTsfABkDzF2"
      },
      "source": [
        "### Topic Model\n",
        "\n",
        "Now it's time to build a LDA-based model by setting values for:\n",
        "- number of topics\n",
        "- alpha\n",
        "- beta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BP5oPuB9GPfv"
      },
      "source": [
        "\n",
        "topics = 3 \n",
        "\n",
        "alpha = 100.0\n",
        "\n",
        "beta = 100.0\n",
        "\n",
        "# Run LDA\n",
        "lda = LatentDirichletAllocation(\n",
        "    n_components=topics, \n",
        "    doc_topic_prior=alpha, \n",
        "    topic_word_prior=beta, \n",
        "    max_iter=25, \n",
        "    learning_method='online', \n",
        "    evaluate_every=1,\n",
        "    n_jobs = -1,\n",
        "    random_state=0,\n",
        "    verbose=1)\n",
        "lda.fit(bag_of_words)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLPTKvHnpvM2"
      },
      "source": [
        "Explore topics:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtFvTVvvd6Sk"
      },
      "source": [
        "no_top_words = 10\n",
        "no_top_documents = 5\n",
        "\n",
        "doc_topics = lda.transform(bag_of_words)\n",
        "topics = lda.components_\n",
        "\n",
        "print(\"LDA Topics\")\n",
        "for topic_idx, topic in enumerate(topics):\n",
        "    print(\"-\"*30)\n",
        "    print(\" Topic \",(topic_idx),\" :\")\n",
        "    print(\"[\",\" | \".join([dictionary[i]\n",
        "                    for i in topic.argsort()[:-no_top_words - 1:-1]]),\"]\")\n",
        "    top_doc_indices = np.argsort( doc_topics[:,topic_idx] )[::-1][0:no_top_documents]\n",
        "    for doc_index in top_doc_indices:\n",
        "        row_index = doc_index +1\n",
        "        print(\"[\",doc_index,\"] (\",rows[row_index][0],\") \\'\",rows[row_index][1],\"\\'\", [ \"{0:.5f}\".format(weight) for weight in doc_topics[doc_index]])\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80GMbeBiV9T7"
      },
      "source": [
        "### Doc-Topic Matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiuAQnr_Uf7s"
      },
      "source": [
        "from IPython.display import display, HTML\n",
        "import pandas as pd\n",
        "#pd.set_option('display.max_columns', None)\n",
        "\n",
        "topicnames = [\"topic\"+ str(x) for x in range(0, lda.n_components)]\n",
        "\n",
        "df = pd.DataFrame(doc_topics, \n",
        "                  columns=topicnames, \n",
        "                  index=dataset_df['TITLE'].tolist())\n",
        "\n",
        "data_table.DataTable(df, num_rows_per_page=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ErZIstoo9fz"
      },
      "source": [
        "### Topic-Word Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHw3jrtCo_Qn"
      },
      "source": [
        "# Topic-Keyword Matrix\n",
        "df_topic_keywords = pd.DataFrame(lda.components_ / lda.components_.sum(axis=1)[:, np.newaxis])\n",
        "\n",
        "# Assign Column and Index\n",
        "df_topic_keywords.columns = dictionary\n",
        "df_topic_keywords.index = topicnames\n",
        "\n",
        "# View\n",
        "data_table.DataTable(df_topic_keywords, num_rows_per_page=10)\n",
        "#df_topic_keywords.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJJrdIJcse74"
      },
      "source": [
        "### Top10 Words per Topic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dal95LEbshti"
      },
      "source": [
        "def show_topics(vectorizer=tf_vectorizer, lda_model=lda, n_words=20):\n",
        "    keywords = np.array(vectorizer.get_feature_names())\n",
        "    topic_keywords = []\n",
        "    for topic_weights in lda_model.components_:\n",
        "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
        "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
        "    return topic_keywords\n",
        "topic_keywords = show_topics(vectorizer=tf_vectorizer, lda_model=lda, n_words=10)\n",
        "\n",
        "# Topic - Keywords Dataframe\n",
        "df_topic_keywords = pd.DataFrame(topic_keywords)\n",
        "df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
        "df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
        "data_table.DataTable(df_topic_keywords, num_rows_per_page=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtzhJ6BZNQ2z"
      },
      "source": [
        "### Topic Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4zVOinlMBRR"
      },
      "source": [
        "text = \"we develop a project to process models in a large-scale\" #@param {type:\"string\"}\n",
        "\n",
        "print(\"Topic Distribution: \", lda.transform(tf_vectorizer.transform([text])))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJA7WxT6i7kw"
      },
      "source": [
        "### Diagnose model performance\n",
        "\n",
        "A model with higher log-likelihood and lower perplexity (exp(-1. * log-likelihood per word)) is considered to be good."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FSd9SJQjJJ7"
      },
      "source": [
        "# Log Likelyhood: Higher the better\n",
        "print(\"Log Likelihood: \", lda.score(bag_of_words))\n",
        "\n",
        "# Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
        "print(\"Perplexity: \", lda.perplexity(bag_of_words))\n",
        "\n",
        "# See model parameters\n",
        "print(lda)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtHC50JzmoHA"
      },
      "source": [
        "Determine the best LDA model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdUhxRKlmqhM"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define Search Param\n",
        "search_params = {'n_components': [5, 10, 15], 'doc_topic_prior': [.1, .3, .5], 'topic_word_prior': [.01, .03, .05]}\n",
        "\n",
        "# Init the Model\n",
        "lda = LatentDirichletAllocation(max_iter=5, learning_method='online', learning_offset=50.,random_state=0)\n",
        "\n",
        "# Init Grid Search Class\n",
        "model = GridSearchCV(lda, param_grid=search_params)\n",
        "\n",
        "# Do the Grid Search\n",
        "model.fit(bag_of_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5ZYRmGSneuR"
      },
      "source": [
        "Read the best configuration:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-emluQQtng3R"
      },
      "source": [
        "# Best Model\n",
        "best_lda_model = model.best_estimator_\n",
        "\n",
        "# Model Parameters\n",
        "print(\"Best Model's Params: \", model.best_params_)\n",
        "\n",
        "# Log Likelihood Score\n",
        "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
        "\n",
        "# Perplexity\n",
        "print(\"Model Perplexity: \", best_lda_model.perplexity(bag_of_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "654ZQ4GEXso2"
      },
      "source": [
        "### Topic-based Document Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrmK-UcmXw9m"
      },
      "source": [
        "# Compute Jensen Shannon Divergence\n",
        "from scipy.spatial import distance\n",
        "\n",
        "for i1,d1 in enumerate(doc_topics[0:10]):\n",
        "   for i2,d2 in enumerate(doc_topics[0:10]):\n",
        "      print(rows[i1+1][1],\"-\", rows[i2+1][1],\":\", 1-distance.jensenshannon(d1, d2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0bYI-bEPG_g"
      },
      "source": [
        "You can tune the `CounterVectorizer` module to clean the input text, or use an already processed corpus. \n",
        "\n",
        "Save a copy of this [file](https://goo.gl/RF4cWB) in the `Google Colab/` folder of your Google Drive."
      ]
    }
  ]
}